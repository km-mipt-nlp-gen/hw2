{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка окружения"
      ],
      "metadata": {
        "id": "H7oG_dTWfllX"
      },
      "id": "H7oG_dTWfllX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установка пакетов и импорт зависимостей"
      ],
      "metadata": {
        "id": "4JQkfTdQoxQK"
      },
      "id": "4JQkfTdQoxQK"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install pandas\n",
        "!pip install flask\n",
        "!pip install pyngrok\n",
        "!pip install pytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqTb75Y-2qUS",
        "outputId": "67a8a3c8-5a79-497c-bd79-f57d3e645ff9"
      },
      "id": "eqTb75Y-2qUS",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.5)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (7.4.4)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "35fc1c4b-8327-4498-9b56-5f2680c93aaf",
      "metadata": {
        "id": "35fc1c4b-8327-4498-9b56-5f2680c93aaf"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import importlib\n",
        "import os\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "from getpass import getpass\n",
        "import logging\n",
        "\n",
        "from typing import Tuple, List\n",
        "\n",
        "from google.colab import drive\n",
        "from joblib import dump, load\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Настройка окружения"
      ],
      "metadata": {
        "id": "qbYo3xdzojRD"
      },
      "id": "qbYo3xdzojRD"
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "743P1zlT_rH2",
        "outputId": "4a842b74-6709-4769-d432-c457c7917e28"
      },
      "id": "743P1zlT_rH2",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Клонирование исходных кодов проекта"
      ],
      "metadata": {
        "id": "CPXCziB0CF4g"
      },
      "id": "CPXCziB0CF4g"
    },
    {
      "cell_type": "code",
      "source": [
        "def clone_pull_github_src(pull: bool = True):\n",
        "    \"\"\"\n",
        "    Клонирует или обновляет репозиторий GitHub в локальный каталог для последующей работы.\n",
        "\n",
        "    Parameters:\n",
        "    pull (bool): Указывает, следует ли выполнять pull для существующего репозитория. Если True, выполняется git pull.\n",
        "                 Если False, репозиторий клонируется в указанный каталог.\n",
        "\n",
        "    Returns:\n",
        "    Constants: Экземпляр класса Constants, содержащий константы проекта.\n",
        "    \"\"\"\n",
        "    WORKSPACE_PATH = '/content/drive/MyDrive/docs/keepForever/mipt/nlp/hw_4sem/'\n",
        "    WORKSPACE_TMP = WORKSPACE_PATH + '/tmp/'\n",
        "    GIT_HUB_PROJECT_PATH = WORKSPACE_PATH + 'code/'\n",
        "\n",
        "    token = getpass('Введите GitHub token: ')\n",
        "    repo_url = 'https://github.com/km-mipt-nlp-gen/hw2.git'\n",
        "    repo_url_with_token = repo_url.replace('https://', f'https://{token}@')\n",
        "\n",
        "    os.chdir(GIT_HUB_PROJECT_PATH)\n",
        "\n",
        "    if pull:\n",
        "        !git pull origin main\n",
        "    else:\n",
        "        !git clone {repo_url_with_token} \"$GIT_HUB_PROJECT_PATH\"\n",
        "\n",
        "    del token\n",
        "\n",
        "    sys.path.append(f\"{GIT_HUB_PROJECT_PATH}/web_app/src/\")\n",
        "    import constants_module\n",
        "    importlib.reload(constants_module)\n",
        "    from constants_module import Constants\n",
        "\n",
        "    return Constants()\n",
        "\n",
        "constants = clone_pull_github_src()\n",
        "from constants_module import Constants\n",
        "from chat_util_module import ChatUtil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9sNEaZmx9uu",
        "outputId": "1122e3a0-ed1a-4466-ff3f-b518ebdca4bd"
      },
      "id": "H9sNEaZmx9uu",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Введите GitHub token: ··········\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 6 (delta 4), reused 6 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6), 504 bytes | 7.00 KiB/s, done.\n",
            "From https://github.com/km-mipt-nlp-gen/hw2\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   c905009..52bcc1d  main       -> origin/main\n",
            "Updating c905009..52bcc1d\n",
            "Fast-forward\n",
            " web_app/src/chat_controller_module.py | 39 \u001b[31m---------------------------------------\u001b[m\n",
            " web_app/src/chat_service_module.py    |  2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 2 files changed, 1 insertion(+), 40 deletions(-)\n",
            "DEVICE: cuda:0\n",
            "Число процессов для использования: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обновление модулей"
      ],
      "metadata": {
        "id": "QVl_F0MRDVz1"
      },
      "id": "QVl_F0MRDVz1"
    },
    {
      "cell_type": "code",
      "source": [
        "def reload_modules(constants: Constants) -> Tuple[Constants, ChatUtil]:\n",
        "    \"\"\"\n",
        "    Перезагружает модули проекта для обновления изменений в коде без перезапуска среды выполнения.\n",
        "\n",
        "    Parameters:\n",
        "    constants (Constants): Экземпляр класса Constants, содержащий постоянные поля.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[Constants, ChatUtil]: Кортеж, содержащий обновленный экземпляр класса Constants и экземпляр ChatUtil.\n",
        "    \"\"\"\n",
        "    import sys\n",
        "    sys.path.append(f\"{constants.GIT_HUB_PROJECT_PATH}/web_app/src/\")\n",
        "    sys.path.append(f\"{constants.GIT_HUB_PROJECT_PATH}/ml/src/train/\")\n",
        "\n",
        "    import importlib\n",
        "    import chat_repository_module\n",
        "    import chat_service_module\n",
        "    import chat_controller_module\n",
        "    import chat_util_module\n",
        "    import constants_module\n",
        "    import models_zoo_module\n",
        "    import run_web_app_script\n",
        "\n",
        "    importlib.reload(chat_repository_module)\n",
        "    importlib.reload(chat_service_module)\n",
        "    importlib.reload(chat_controller_module)\n",
        "    importlib.reload(chat_util_module)\n",
        "    importlib.reload(constants_module)\n",
        "    importlib.reload(models_zoo_module)\n",
        "    importlib.reload(run_web_app_script)\n",
        "\n",
        "    from constants_module import Constants\n",
        "    from chat_util_module import ChatUtil\n",
        "    from chat_repository_module import ChatRepository\n",
        "    from chat_service_module import ChatService\n",
        "    from chat_controller_module import ChatController\n",
        "    from run_web_app_script import run_web_app\n",
        "\n",
        "    constants = Constants()\n",
        "\n",
        "    return constants, ChatUtil(logging.DEBUG, constants), run_web_app\n",
        "\n",
        "constants, chat_util, run_web_app = reload_modules(constants)\n",
        "preprocessed_data = load(constants.PROCESSED_QA_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nqNrd_yYxpr",
        "outputId": "6ad50593-bde4-40f9-f06a-01869177e4e3"
      },
      "id": "2nqNrd_yYxpr",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE: cuda:0\n",
            "Число процессов для использования: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web-приложение"
      ],
      "metadata": {
        "id": "Ihw2gFpz71OG"
      },
      "id": "Ihw2gFpz71OG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Интеграционные тесты Web-приложения"
      ],
      "metadata": {
        "id": "hhv9u-UAZBMx"
      },
      "id": "hhv9u-UAZBMx"
    },
    {
      "cell_type": "code",
      "source": [
        "def run_integration_tests(constants: Constants) -> None:\n",
        "    \"\"\"\n",
        "    Запускает интеграционные тесты веб-приложения.\n",
        "\n",
        "    Parameters:\n",
        "    constants (Constants): Экземпляр класса Constants, содержащий постоянные поля проекта.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    os.chdir(constants.WEB_APP_TEST_PATH)\n",
        "\n",
        "    os.environ['WEB_APP_SRC_PATH'] = constants.WEB_APP_SRC_PATH\n",
        "\n",
        "    command = [\n",
        "        \"pytest\", \"-vv\",\n",
        "        constants.TEST_SCRIPT_PATH\n",
        "    ]\n",
        "\n",
        "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "    print(result.stdout)\n",
        "    print(result.stderr)\n",
        "\n",
        "run_integration_tests(constants)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmgW06n3p--J",
        "outputId": "108ecb8e-a2e3-43fb-ffef-fad949bab012"
      },
      "id": "zmgW06n3p--J",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================= test session starts ========================================\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.4.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/drive/MyDrive/docs/keepForever/mipt/nlp/hw_4sem/code/web_app/test\n",
            "plugins: anyio-3.7.1\n",
            "collecting ... collected 3 items\n",
            "\n",
            "test.py::test_that_expected_length_when_find_top_3_times[/gpt2-6] PASSED                     [ 33%]\n",
            "test.py::test_that_expected_length_when_find_top[/gpt2-2] PASSED                             [ 66%]\n",
            "test.py::test_that_clear_chat_when_find_top_then_clear[/gpt2] PASSED                         [100%]\n",
            "\n",
            "======================================== 3 passed in 6.04s =========================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Старт Web-приложения"
      ],
      "metadata": {
        "id": "KWM9uECIh1oT"
      },
      "id": "KWM9uECIh1oT"
    },
    {
      "cell_type": "code",
      "source": [
        "run_web_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8sqn-mrJa3w",
        "outputId": "5fdf5b76-9489-47d3-ed95-33f1865c1278"
      },
      "id": "K8sqn-mrJa3w",
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module: * ngrok tunnel \"https://b91e-34-126-65-49.ngrok-free.app\" -> \"http://127.0.0.1:5000/\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Flask 'chat_controller_module'>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app 'chat_controller_module'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo fuser -k 4040/tcp # прервать процесс сервиса\n",
        "# !sudo fuser -k 5000/tcp\n",
        "# !ps aux | grep ngrok\n",
        "# !ps aux | grep 4040"
      ],
      "metadata": {
        "id": "qCaV9PlTIzXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175db2b7-a262-4600-dac1-a3136051cc39"
      },
      "id": "qCaV9PlTIzXB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4040/tcp:            29352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Запросы к серверу (через облачный туннель)"
      ],
      "metadata": {
        "id": "6nx0TalkiFmR"
      },
      "id": "6nx0TalkiFmR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Одиночный вопрос и ответ"
      ],
      "metadata": {
        "id": "YyCxqyFms6qc"
      },
      "id": "YyCxqyFms6qc"
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://b91e-34-126-65-49.ngrok-free.app'\n",
        "\n",
        "def process_queries(query_user_pairs: List[Tuple[str, str]], resource_url: str) -> None:\n",
        "    \"\"\"\n",
        "    Реализация режима одиночного запроса и ответа.\n",
        "    Обрабатывает серию запросов к API и выводит отформатированные ответы:\n",
        "    - для пары пользователь - реплика формирует запрос;\n",
        "    - далее отправляет запрос на 'top_cos_sim_bi_cr', 'top_l2_bi_cr', 'top_l2_psa_bi_cr', 'top_cr';\n",
        "    - выводит ответы напротив каждого запроса;\n",
        "    - очищает чат после каждого ответа.\n",
        "\n",
        "    Parameters:\n",
        "    query_user_pairs (List[Tuple[str, str]]): Список кортежей, содержащих пары пользователь-запрос.\n",
        "    resource_url (str): Базовый URL ресурса, к которому будут отправляться запросы.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    endpoints = [\n",
        "        'gpt2'\n",
        "    ]\n",
        "    clear_endpoint = '/clear'\n",
        "\n",
        "    for index, (user, query) in enumerate(query_user_pairs, start=1):\n",
        "        data = {\n",
        "            \"query\": query,\n",
        "            \"user\": user\n",
        "        }\n",
        "\n",
        "        for endpoint in endpoints:\n",
        "            full_url = f'{resource_url}/{endpoint}'\n",
        "            response = requests.post(full_url, json=data)\n",
        "\n",
        "            formatted_response = json.dumps(response.json(), ensure_ascii=False, indent=2) if response.ok else response.text\n",
        "            print(f'{index} \"/{endpoint}\" запрос:\"{json.dumps(data, ensure_ascii=False)}\" > ответ:\"{formatted_response}\"')\n",
        "\n",
        "            requests.delete(f'{resource_url}{clear_endpoint}')\n",
        "\n",
        "            if endpoint != endpoints[-1]:\n",
        "                print('---')\n",
        "\n",
        "        if index != len(query_user_pairs):\n",
        "            print('=================================================================\\n')\n",
        "\n",
        "# вопрос - ответ (без сохранения контекста после ответа)\n",
        "query_user_pairs = [\n",
        "    ('Marge Simpson', 'Kids, you want to go Christmas shopping?'),\n",
        "    ('Marge Simpson', 'Ready for Christmas, shopaholics?'),\n",
        "    ('Ivan Ivanov', 'Ready for Christmas, shopaholics?'),\n",
        "    ('Ivan Ivanov', 'Write a letter to Santa to Russia to Veliky Ustyug.'),\n",
        "    ('Bart Simpson', 'Good one, Dad.'),\n",
        "    ('Bart Simpson', 'Awesome, Dad.'),\n",
        "    ('Igor Pokrishkin', 'Awesome, Dad.'),\n",
        "    ('Igor Pokrishkin', 'Top-notch skill, Dad.')\n",
        "]\n",
        "\n",
        "process_queries(query_user_pairs, url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozWoV0vxwZyp",
        "outputId": "bb510790-de0a-456a-9bb2-0acf9b1c40cc"
      },
      "id": "ozWoV0vxwZyp",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Kids, you want to go Christmas shopping?\n",
            "DEBUG:chat_util_module:User: Marge Simpson\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Marge Simpson: Kids, you want to go Christmas shopping?\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: It's a good thing Dad's got a big Santa Clause.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Marge Simpson: Kids, you want to go Christmas shopping?', \"It's a good thing Dad's got a big Santa Clause.\"]\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:31] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \"/gpt2\" запрос:\"{\"query\": \"Kids, you want to go Christmas shopping?\", \"user\": \"Marge Simpson\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Marge Simpson: Kids, you want to go Christmas shopping?\",\n",
            "    \"It's a good thing Dad's got a big Santa Clause.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:32] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Ready for Christmas, shopaholics?\n",
            "DEBUG:chat_util_module:User: Marge Simpson\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Marge Simpson: Ready for Christmas, shopaholics?\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: (FOUGHT THE TRICK.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Marge Simpson: Ready for Christmas, shopaholics?', '(FOUGHT THE TRICK.']\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:34] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 \"/gpt2\" запрос:\"{\"query\": \"Ready for Christmas, shopaholics?\", \"user\": \"Marge Simpson\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Marge Simpson: Ready for Christmas, shopaholics?\",\n",
            "    \"(FOUGHT THE TRICK.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:35] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Ready for Christmas, shopaholics?\n",
            "DEBUG:chat_util_module:User: Ivan Ivanov\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Ivan Ivanov: Ready for Christmas, shopaholics?\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: You know, the little one.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Ivan Ivanov: Ready for Christmas, shopaholics?', 'You know, the little one.']\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:36] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 \"/gpt2\" запрос:\"{\"query\": \"Ready for Christmas, shopaholics?\", \"user\": \"Ivan Ivanov\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Ivan Ivanov: Ready for Christmas, shopaholics?\",\n",
            "    \"You know, the little one.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:37] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Write a letter to Santa to Russia to Veliky Ustyug.\n",
            "DEBUG:chat_util_module:User: Ivan Ivanov\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Ivan Ivanov: Write a letter to Santa to Russia to Veliky Ustyug.\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: Don't worry about the rest. We're all over this.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Ivan Ivanov: Write a letter to Santa to Russia to Veliky Ustyug.', \"Don't worry about the rest. We're all over this.\"]\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:38] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 \"/gpt2\" запрос:\"{\"query\": \"Write a letter to Santa to Russia to Veliky Ustyug.\", \"user\": \"Ivan Ivanov\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Ivan Ivanov: Write a letter to Santa to Russia to Veliky Ustyug.\",\n",
            "    \"Don't worry about the rest. We're all over this.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:39] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Good one, Dad.\n",
            "DEBUG:chat_util_module:User: Bart Simpson\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Bart Simpson: Good one, Dad.\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: Good one. And a few more of your jokes about me.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Bart Simpson: Good one, Dad.', 'Good one. And a few more of your jokes about me.']\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:40] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 \"/gpt2\" запрос:\"{\"query\": \"Good one, Dad.\", \"user\": \"Bart Simpson\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Bart Simpson: Good one, Dad.\",\n",
            "    \"Good one. And a few more of your jokes about me.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:40] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Awesome, Dad.\n",
            "DEBUG:chat_util_module:User: Bart Simpson\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Bart Simpson: Awesome, Dad.\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: Dad?\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Bart Simpson: Awesome, Dad.', 'Dad?']\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:42] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6 \"/gpt2\" запрос:\"{\"query\": \"Awesome, Dad.\", \"user\": \"Bart Simpson\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Bart Simpson: Awesome, Dad.\",\n",
            "    \"Dad?\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:43] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Awesome, Dad.\n",
            "DEBUG:chat_util_module:User: Igor Pokrishkin\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Igor Pokrishkin: Awesome, Dad.\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: And your sister was killed by a snake.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Igor Pokrishkin: Awesome, Dad.', 'And your sister was killed by a snake.']\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:44] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 \"/gpt2\" запрос:\"{\"query\": \"Awesome, Dad.\", \"user\": \"Igor Pokrishkin\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Igor Pokrishkin: Awesome, Dad.\",\n",
            "    \"And your sister was killed by a snake.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:45] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Top-notch skill, Dad.\n",
            "DEBUG:chat_util_module:User: Igor Pokrishkin\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Igor Pokrishkin: Top-notch skill, Dad.\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: I think you're a good one.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Igor Pokrishkin: Top-notch skill, Dad.', \"I think you're a good one.\"]\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:46] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 \"/gpt2\" запрос:\"{\"query\": \"Top-notch skill, Dad.\", \"user\": \"Igor Pokrishkin\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Igor Pokrishkin: Top-notch skill, Dad.\",\n",
            "    \"I think you're a good one.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:13:47] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Диалог"
      ],
      "metadata": {
        "id": "ohdLsof1tK9H"
      },
      "id": "ohdLsof1tK9H"
    },
    {
      "cell_type": "code",
      "source": [
        "def process_queries(query_user_pairs: List[Tuple[str, str]], resource_url: str) -> None:\n",
        "    \"\"\"\n",
        "    Реализация режима диалога.\n",
        "    Обрабатывает серию запросов к API и выводит состояние чата после каждого запроса:\n",
        "    - для каждого из 'top_cos_sim_bi_cr', 'top_l2_bi_cr', 'top_l2_psa_bi_cr', 'top_cr' выполняет серию запросов;\n",
        "    - запрос состввляется для пары пользователь - реплика;\n",
        "    - после обработки всех запросов - переходит к отправке сообщений на следующий ресурс;\n",
        "    - выводит текущего чата напротив каждого запроса;\n",
        "    - очищает чат после завершения работы с ресурсом (после отправки всех запросов на текущий ресурс).\n",
        "\n",
        "    Parameters:\n",
        "    query_user_pairs (List[Tuple[str, str]]): Список кортежей, содержащих пары пользователь-запрос.\n",
        "    resource_url (str): Базовый URL ресурса, к которому будут отправляться запросы.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    endpoints = [\n",
        "        'gpt2'\n",
        "    ]\n",
        "    clear_endpoint = '/clear'\n",
        "\n",
        "    for endpoint in endpoints:\n",
        "        for index, (user, query) in enumerate(query_user_pairs, start=1):\n",
        "            data = {\n",
        "                \"query\": query,\n",
        "                \"user\": user\n",
        "            }\n",
        "\n",
        "\n",
        "            full_url = f'{resource_url}/{endpoint}'\n",
        "            response = requests.post(full_url, json=data)\n",
        "\n",
        "            formatted_response = json.dumps(response.json(), ensure_ascii=False, indent=2) if response.ok else response.text\n",
        "            print(f'{index} \"/{endpoint}\" запрос:\"{json.dumps(data, ensure_ascii=False)}\" > состояние чата:\"{formatted_response}\"')\n",
        "\n",
        "            if index != len(query_user_pairs):\n",
        "                print('---')\n",
        "\n",
        "        requests.delete(f'{resource_url}{clear_endpoint}')\n",
        "        if endpoint != endpoints[-1]:\n",
        "            print('=================================================================\\n')\n",
        "\n",
        "# вопрос - состояние чата (с сохранением контекста)\n",
        "query_user_pairs = [\n",
        "    ('Postman', 'Your Cristmas Tree.'),\n",
        "    ('Postman', 'From North Pole.')\n",
        "]\n",
        "\n",
        "resource_url =  url\n",
        "process_queries(query_user_pairs, resource_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK8GUiQg5Tey",
        "outputId": "ba8fcada-1a35-46ea-d285-d30ad79e8ae9"
      },
      "id": "BK8GUiQg5Tey",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Your Cristmas Tree.\n",
            "DEBUG:chat_util_module:User: Postman\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Postman: Your Cristmas Tree.\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: I am, Mrs. Marge Simpson...\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Postman: Your Cristmas Tree.', 'I am, Mrs. Marge Simpson...']\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:16:28] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \"/gpt2\" запрос:\"{\"query\": \"Your Cristmas Tree.\", \"user\": \"Postman\"}\" > состояние чата:\"{\n",
            "  \"response\": [\n",
            "    \"Postman: Your Cristmas Tree.\",\n",
            "    \"I am, Mrs. Marge Simpson...\"\n",
            "  ]\n",
            "}\"\n",
            "---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: From North Pole.\n",
            "DEBUG:chat_util_module:User: Postman\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_3: \"Postman: Your Cristmas Tree.\"; R_2: \"I am, Mrs. Marge Simpson...\"; R_1: \"Postman: From North Pole.\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: From North Pole.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Postman: Your Cristmas Tree.', 'I am, Mrs. Marge Simpson...', 'Postman: From North Pole.', 'From North Pole.']\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:16:29] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 \"/gpt2\" запрос:\"{\"query\": \"From North Pole.\", \"user\": \"Postman\"}\" > состояние чата:\"{\n",
            "  \"response\": [\n",
            "    \"Postman: Your Cristmas Tree.\",\n",
            "    \"I am, Mrs. Marge Simpson...\",\n",
            "    \"Postman: From North Pole.\",\n",
            "    \"From North Pole.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 18:16:29] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}