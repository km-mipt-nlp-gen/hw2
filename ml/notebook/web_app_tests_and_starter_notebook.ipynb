{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка окружения"
      ],
      "metadata": {
        "id": "H7oG_dTWfllX"
      },
      "id": "H7oG_dTWfllX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установка пакетов и импорт зависимостей"
      ],
      "metadata": {
        "id": "4JQkfTdQoxQK"
      },
      "id": "4JQkfTdQoxQK"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install pandas\n",
        "!pip install flask\n",
        "!pip install pyngrok\n",
        "!pip install pytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqTb75Y-2qUS",
        "outputId": "ac325b07-b849-4c84-f429-45e052d44257"
      },
      "id": "eqTb75Y-2qUS",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.5-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.5\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (7.4.4)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "35fc1c4b-8327-4498-9b56-5f2680c93aaf",
      "metadata": {
        "id": "35fc1c4b-8327-4498-9b56-5f2680c93aaf"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import importlib\n",
        "import os\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "from getpass import getpass\n",
        "import logging\n",
        "\n",
        "from typing import Tuple, List\n",
        "\n",
        "from google.colab import drive\n",
        "from joblib import dump, load\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Настройка окружения"
      ],
      "metadata": {
        "id": "qbYo3xdzojRD"
      },
      "id": "qbYo3xdzojRD"
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "743P1zlT_rH2",
        "outputId": "f94d31b8-9524-402e-f2c6-1e5d0953c390"
      },
      "id": "743P1zlT_rH2",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Клонирование исходных кодов проекта"
      ],
      "metadata": {
        "id": "CPXCziB0CF4g"
      },
      "id": "CPXCziB0CF4g"
    },
    {
      "cell_type": "code",
      "source": [
        "def clone_pull_github_src(pull: bool = True):\n",
        "    \"\"\"\n",
        "    Клонирует или обновляет репозиторий GitHub в локальный каталог для последующей работы.\n",
        "\n",
        "    Parameters:\n",
        "    pull (bool): Указывает, следует ли выполнять pull для существующего репозитория. Если True, выполняется git pull.\n",
        "                 Если False, репозиторий клонируется в указанный каталог.\n",
        "\n",
        "    Returns:\n",
        "    Constants: Экземпляр класса Constants, содержащий константы проекта.\n",
        "    \"\"\"\n",
        "    WORKSPACE_PATH = '/content/drive/MyDrive/docs/keepForever/mipt/nlp/hw_4sem/'\n",
        "    WORKSPACE_TMP = WORKSPACE_PATH + '/tmp/'\n",
        "    GIT_HUB_PROJECT_PATH = WORKSPACE_PATH + 'code/'\n",
        "\n",
        "    token = getpass('Введите GitHub token: ')\n",
        "    repo_url = 'https://github.com/km-mipt-nlp-gen/hw2.git'\n",
        "    repo_url_with_token = repo_url.replace('https://', f'https://{token}@')\n",
        "\n",
        "    os.chdir(GIT_HUB_PROJECT_PATH)\n",
        "\n",
        "    if pull:\n",
        "        !git pull origin main\n",
        "    else:\n",
        "        !git clone {repo_url_with_token} \"$GIT_HUB_PROJECT_PATH\"\n",
        "\n",
        "    del token\n",
        "\n",
        "    sys.path.append(f\"{GIT_HUB_PROJECT_PATH}/web_app/src/\")\n",
        "    import constants_module\n",
        "    importlib.reload(constants_module)\n",
        "    from constants_module import Constants\n",
        "\n",
        "    return Constants()\n",
        "\n",
        "constants = clone_pull_github_src()\n",
        "from constants_module import Constants\n",
        "from chat_util_module import ChatUtil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9sNEaZmx9uu",
        "outputId": "1cfce143-e899-4119-a97a-2eb680713ed3"
      },
      "id": "H9sNEaZmx9uu",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Введите GitHub token: ··········\n",
            "From https://github.com/km-mipt-nlp-gen/hw2\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "DEVICE: cuda:0\n",
            "Число процессов для использования: 8\n",
            "DEVICE: cuda:0\n",
            "Число процессов для использования: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обновление модулей"
      ],
      "metadata": {
        "id": "QVl_F0MRDVz1"
      },
      "id": "QVl_F0MRDVz1"
    },
    {
      "cell_type": "code",
      "source": [
        "def reload_modules(constants: Constants) -> Tuple[Constants, ChatUtil]:\n",
        "    \"\"\"\n",
        "    Перезагружает модули проекта для обновления изменений в коде без перезапуска среды выполнения.\n",
        "\n",
        "    Parameters:\n",
        "    constants (Constants): Экземпляр класса Constants, содержащий постоянные поля.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[Constants, ChatUtil]: Кортеж, содержащий обновленный экземпляр класса Constants и экземпляр ChatUtil.\n",
        "    \"\"\"\n",
        "    import sys\n",
        "    sys.path.append(f\"{constants.GIT_HUB_PROJECT_PATH}/web_app/src/\")\n",
        "    sys.path.append(f\"{constants.GIT_HUB_PROJECT_PATH}/ml/src/train/\")\n",
        "\n",
        "    import importlib\n",
        "    import chat_repository_module\n",
        "    import chat_service_module\n",
        "    import chat_controller_module\n",
        "    import chat_util_module\n",
        "    import constants_module\n",
        "    import models_zoo_module\n",
        "    import run_web_app_script\n",
        "\n",
        "    importlib.reload(chat_repository_module)\n",
        "    importlib.reload(chat_service_module)\n",
        "    importlib.reload(chat_controller_module)\n",
        "    importlib.reload(chat_util_module)\n",
        "    importlib.reload(constants_module)\n",
        "    importlib.reload(models_zoo_module)\n",
        "    importlib.reload(run_web_app_script)\n",
        "\n",
        "    from constants_module import Constants\n",
        "    from chat_util_module import ChatUtil\n",
        "    from chat_repository_module import ChatRepository\n",
        "    from chat_service_module import ChatService\n",
        "    from chat_controller_module import ChatController\n",
        "    from run_web_app_script import run_web_app\n",
        "\n",
        "    constants = Constants()\n",
        "\n",
        "    return constants, ChatUtil(logging.DEBUG, constants), run_web_app\n",
        "\n",
        "constants, chat_util, run_web_app = reload_modules(constants)\n",
        "preprocessed_data = load(constants.PROCESSED_QA_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nqNrd_yYxpr",
        "outputId": "caffeeb5-585a-43cc-c2f4-699b8f6e54e7"
      },
      "id": "2nqNrd_yYxpr",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE: cuda:0\n",
            "Число процессов для использования: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web-приложение"
      ],
      "metadata": {
        "id": "Ihw2gFpz71OG"
      },
      "id": "Ihw2gFpz71OG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Интеграционные тесты Web-приложения"
      ],
      "metadata": {
        "id": "hhv9u-UAZBMx"
      },
      "id": "hhv9u-UAZBMx"
    },
    {
      "cell_type": "code",
      "source": [
        "def run_integration_tests(constants: Constants) -> None:\n",
        "    \"\"\"\n",
        "    Запускает интеграционные тесты веб-приложения.\n",
        "\n",
        "    Parameters:\n",
        "    constants (Constants): Экземпляр класса Constants, содержащий постоянные поля проекта.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    os.chdir(constants.WEB_APP_TEST_PATH)\n",
        "\n",
        "    os.environ['WEB_APP_SRC_PATH'] = constants.WEB_APP_SRC_PATH\n",
        "\n",
        "    command = [\n",
        "        \"pytest\", \"-vv\",\n",
        "        constants.TEST_SCRIPT_PATH\n",
        "    ]\n",
        "\n",
        "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "    print(result.stdout)\n",
        "    print(result.stderr)\n",
        "\n",
        "run_integration_tests(constants)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmgW06n3p--J",
        "outputId": "5081dcb6-9b56-4c3c-df11-803e3b6bb5d5"
      },
      "id": "zmgW06n3p--J",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================= test session starts ========================================\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.4.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/drive/MyDrive/docs/keepForever/mipt/nlp/hw_4sem/code/web_app/test\n",
            "plugins: anyio-3.7.1\n",
            "collecting ... collected 3 items\n",
            "\n",
            "test.py::test_that_expected_length_when_find_top_3_times[/gpt2-6] PASSED                     [ 33%]\n",
            "test.py::test_that_expected_length_when_find_top[/gpt2-2] PASSED                             [ 66%]\n",
            "test.py::test_that_clear_chat_when_find_top_then_clear[/gpt2] PASSED                         [100%]\n",
            "\n",
            "======================================== 3 passed in 39.72s ========================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Старт Web-приложения"
      ],
      "metadata": {
        "id": "KWM9uECIh1oT"
      },
      "id": "KWM9uECIh1oT"
    },
    {
      "cell_type": "code",
      "source": [
        "run_web_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8sqn-mrJa3w",
        "outputId": "edb0c07d-fec0-41cb-a6b1-1bf10b876ae9"
      },
      "id": "K8sqn-mrJa3w",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module: * ngrok tunnel \"https://9fcc-35-221-227-29.ngrok-free.app\" -> \"http://127.0.0.1:5000/\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Flask 'chat_controller_module'>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo fuser -k 4040/tcp # прервать процесс сервиса\n",
        "# !sudo fuser -k 5000/tcp\n",
        "# !ps aux | grep ngrok\n",
        "# !ps aux | grep 4040\n",
        "# !sudo fuser -k 7832/tcp"
      ],
      "metadata": {
        "id": "qCaV9PlTIzXB"
      },
      "id": "qCaV9PlTIzXB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Запросы к серверу (через облачный туннель)"
      ],
      "metadata": {
        "id": "6nx0TalkiFmR"
      },
      "id": "6nx0TalkiFmR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Одиночный вопрос и ответ"
      ],
      "metadata": {
        "id": "YyCxqyFms6qc"
      },
      "id": "YyCxqyFms6qc"
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://9fcc-35-221-227-29.ngrok-free.app\"\n",
        "\n",
        "def process_queries(query_user_pairs: List[Tuple[str, str]], resource_url: str) -> None:\n",
        "    \"\"\"\n",
        "    Реализация режима одиночного запроса и ответа.\n",
        "    Обрабатывает серию запросов к API и выводит отформатированные ответы:\n",
        "    - для пары пользователь - реплика формирует запрос;\n",
        "    - далее отправляет запрос на ресурс;\n",
        "    - выводит ответы напротив каждого запроса;\n",
        "    - очищает чат после каждого ответа.\n",
        "\n",
        "    Parameters:\n",
        "    query_user_pairs (List[Tuple[str, str]]): Список кортежей, содержащих пары пользователь-запрос.\n",
        "    resource_url (str): Базовый URL ресурса, к которому будут отправляться запросы.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    endpoints = [\n",
        "        'gpt2'\n",
        "    ]\n",
        "    clear_endpoint = '/clear'\n",
        "\n",
        "    for index, (user, query) in enumerate(query_user_pairs, start=1):\n",
        "        data = {\n",
        "            \"query\": query,\n",
        "            \"user\": user\n",
        "        }\n",
        "\n",
        "        for endpoint in endpoints:\n",
        "            full_url = f'{resource_url}/{endpoint}'\n",
        "            response = requests.post(full_url, json=data)\n",
        "\n",
        "            formatted_response = json.dumps(response.json(), ensure_ascii=False, indent=2) if response.ok else response.text\n",
        "            print(f'{index} \"/{endpoint}\" запрос:\"{json.dumps(data, ensure_ascii=False)}\" > ответ:\"{formatted_response}\"')\n",
        "\n",
        "            requests.delete(f'{resource_url}{clear_endpoint}')\n",
        "\n",
        "            if endpoint != endpoints[-1]:\n",
        "                print('---')\n",
        "\n",
        "        if index != len(query_user_pairs):\n",
        "            print('=================================================================\\n')\n",
        "\n",
        "# вопрос - ответ (без сохранения контекста после ответа)\n",
        "query_user_pairs = [\n",
        "    ('Marge Simpson', 'Kids, you want to go Christmas shopping?'),\n",
        "    ('Marge Simpson', 'Ready for Christmas, shopaholics?'),\n",
        "    ('Ivan Ivanov', 'Ready for Christmas, shopaholics?'),\n",
        "    ('Ivan Ivanov', 'Write a letter to Santa to Russia to Veliky Ustyug.'),\n",
        "    ('Bart Simpson', 'Good one, Dad.'),\n",
        "    ('Bart Simpson', 'Awesome, Dad.'),\n",
        "    ('Igor Pokrishkin', 'Awesome, Dad.'),\n",
        "    ('Igor Pokrishkin', 'Top-notch skill, Dad.')\n",
        "]\n",
        "\n",
        "process_queries(query_user_pairs, url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozWoV0vxwZyp",
        "outputId": "0c565f8e-4f58-46a6-ea4a-64c5a7a07362"
      },
      "id": "ozWoV0vxwZyp",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Kids, you want to go Christmas shopping?\n",
            "DEBUG:chat_util_module:User: Marge Simpson\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Marge Simpson: Kids, you want to go Christmas shopping?\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: Lisa Simpson: It's a good thing Dad's got a big Santa Clause.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Marge Simpson: Kids, you want to go Christmas shopping?', \"Lisa Simpson: It's a good thing Dad's got a big Santa Clause.\"]\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:35] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \"/gpt2\" запрос:\"{\"query\": \"Kids, you want to go Christmas shopping?\", \"user\": \"Marge Simpson\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Marge Simpson: Kids, you want to go Christmas shopping?\",\n",
            "    \"Lisa Simpson: It's a good thing Dad's got a big Santa Clause.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:35] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Ready for Christmas, shopaholics?\n",
            "DEBUG:chat_util_module:User: Marge Simpson\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Marge Simpson: Ready for Christmas, shopaholics?\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: Lisa Simpson: Not a little Christmas surprise here, Simpson!\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Marge Simpson: Ready for Christmas, shopaholics?', 'Lisa Simpson: Not a little Christmas surprise here, Simpson!']\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:38] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 \"/gpt2\" запрос:\"{\"query\": \"Ready for Christmas, shopaholics?\", \"user\": \"Marge Simpson\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Marge Simpson: Ready for Christmas, shopaholics?\",\n",
            "    \"Lisa Simpson: Not a little Christmas surprise here, Simpson!\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:38] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Ready for Christmas, shopaholics?\n",
            "DEBUG:chat_util_module:User: Ivan Ivanov\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Ivan Ivanov: Ready for Christmas, shopaholics?\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: Lisa Simpson: You know, the little one.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Ivan Ivanov: Ready for Christmas, shopaholics?', 'Lisa Simpson: You know, the little one.']\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:40] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 \"/gpt2\" запрос:\"{\"query\": \"Ready for Christmas, shopaholics?\", \"user\": \"Ivan Ivanov\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Ivan Ivanov: Ready for Christmas, shopaholics?\",\n",
            "    \"Lisa Simpson: You know, the little one.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:40] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Write a letter to Santa to Russia to Veliky Ustyug.\n",
            "DEBUG:chat_util_module:User: Ivan Ivanov\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Ivan Ivanov: Write a letter to Santa to Russia to Veliky Ustyug.\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: Lisa Simpson: Don't worry about the rest. We're all over this.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Ivan Ivanov: Write a letter to Santa to Russia to Veliky Ustyug.', \"Lisa Simpson: Don't worry about the rest. We're all over this.\"]\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:41] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 \"/gpt2\" запрос:\"{\"query\": \"Write a letter to Santa to Russia to Veliky Ustyug.\", \"user\": \"Ivan Ivanov\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Ivan Ivanov: Write a letter to Santa to Russia to Veliky Ustyug.\",\n",
            "    \"Lisa Simpson: Don't worry about the rest. We're all over this.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:42] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Good one, Dad.\n",
            "DEBUG:chat_util_module:User: Bart Simpson\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Bart Simpson: Good one, Dad.\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: Lisa Simpson: Good one. And a few more of your jokes about me.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Bart Simpson: Good one, Dad.', 'Lisa Simpson: Good one. And a few more of your jokes about me.']\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:43] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 \"/gpt2\" запрос:\"{\"query\": \"Good one, Dad.\", \"user\": \"Bart Simpson\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Bart Simpson: Good one, Dad.\",\n",
            "    \"Lisa Simpson: Good one. And a few more of your jokes about me.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:43] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Awesome, Dad.\n",
            "DEBUG:chat_util_module:User: Bart Simpson\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Bart Simpson: Awesome, Dad.\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: Lisa Simpson: What was that?\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Bart Simpson: Awesome, Dad.', 'Lisa Simpson: What was that?']\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:45] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6 \"/gpt2\" запрос:\"{\"query\": \"Awesome, Dad.\", \"user\": \"Bart Simpson\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Bart Simpson: Awesome, Dad.\",\n",
            "    \"Lisa Simpson: What was that?\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:46] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Awesome, Dad.\n",
            "DEBUG:chat_util_module:User: Igor Pokrishkin\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Igor Pokrishkin: Awesome, Dad.\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: Lisa Simpson: Don't worry Dad, we've got a little plan for this one.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Igor Pokrishkin: Awesome, Dad.', \"Lisa Simpson: Don't worry Dad, we've got a little plan for this one.\"]\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:47] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 \"/gpt2\" запрос:\"{\"query\": \"Awesome, Dad.\", \"user\": \"Igor Pokrishkin\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Igor Pokrishkin: Awesome, Dad.\",\n",
            "    \"Lisa Simpson: Don't worry Dad, we've got a little plan for this one.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:48] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Top-notch skill, Dad.\n",
            "DEBUG:chat_util_module:User: Igor Pokrishkin\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Igor Pokrishkin: Top-notch skill, Dad.\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: Lisa Simpson: I think you're a good one.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Igor Pokrishkin: Top-notch skill, Dad.', \"Lisa Simpson: I think you're a good one.\"]\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:49] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 \"/gpt2\" запрос:\"{\"query\": \"Top-notch skill, Dad.\", \"user\": \"Igor Pokrishkin\"}\" > ответ:\"{\n",
            "  \"response\": [\n",
            "    \"Igor Pokrishkin: Top-notch skill, Dad.\",\n",
            "    \"Lisa Simpson: I think you're a good one.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:49] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Диалог"
      ],
      "metadata": {
        "id": "ohdLsof1tK9H"
      },
      "id": "ohdLsof1tK9H"
    },
    {
      "cell_type": "code",
      "source": [
        "def process_queries(query_user_pairs: List[Tuple[str, str]], resource_url: str) -> None:\n",
        "    \"\"\"\n",
        "    Реализация режима диалога.\n",
        "    Обрабатывает серию запросов к API и выводит состояние чата после каждого запроса:\n",
        "    - для каждого ресурса выполняет серию запросов;\n",
        "    - запрос состввляется для пары пользователь - реплика;\n",
        "    - после обработки всех запросов - переходит к отправке сообщений на следующий ресурс;\n",
        "    - выводит текущего чата напротив каждого запроса;\n",
        "    - очищает чат после завершения работы с ресурсом (после отправки всех запросов на текущий ресурс).\n",
        "\n",
        "    Parameters:\n",
        "    query_user_pairs (List[Tuple[str, str]]): Список кортежей, содержащих пары пользователь-запрос.\n",
        "    resource_url (str): Базовый URL ресурса, к которому будут отправляться запросы.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    endpoints = [\n",
        "        'gpt2'\n",
        "    ]\n",
        "    clear_endpoint = '/clear'\n",
        "\n",
        "    for endpoint in endpoints:\n",
        "        for index, (user, query) in enumerate(query_user_pairs, start=1):\n",
        "            data = {\n",
        "                \"query\": query,\n",
        "                \"user\": user\n",
        "            }\n",
        "\n",
        "\n",
        "            full_url = f'{resource_url}/{endpoint}'\n",
        "            response = requests.post(full_url, json=data)\n",
        "\n",
        "            formatted_response = json.dumps(response.json(), ensure_ascii=False, indent=2) if response.ok else response.text\n",
        "            print(f'{index} \"/{endpoint}\" запрос:\"{json.dumps(data, ensure_ascii=False)}\" > состояние чата:\"{formatted_response}\"')\n",
        "\n",
        "            if index != len(query_user_pairs):\n",
        "                print('---')\n",
        "\n",
        "        requests.delete(f'{resource_url}{clear_endpoint}')\n",
        "        if endpoint != endpoints[-1]:\n",
        "            print('=================================================================\\n')\n",
        "\n",
        "# вопрос - состояние чата (с сохранением контекста)\n",
        "query_user_pairs = [\n",
        "    ('Postman', 'Your Cristmas Tree.'),\n",
        "    ('Postman', 'From North Pole.')\n",
        "]\n",
        "\n",
        "resource_url =  url\n",
        "process_queries(query_user_pairs, resource_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK8GUiQg5Tey",
        "outputId": "537fed20-f1de-4e17-dbef-9d299f7dc94c"
      },
      "id": "BK8GUiQg5Tey",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: Your Cristmas Tree.\n",
            "DEBUG:chat_util_module:User: Postman\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_1: \"Postman: Your Cristmas Tree.\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: Lisa Simpson: I have no problem with that. But I'm concerned with the other tree.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Postman: Your Cristmas Tree.', \"Lisa Simpson: I have no problem with that. But I'm concerned with the other tree.\"]\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:50] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \"/gpt2\" запрос:\"{\"query\": \"Your Cristmas Tree.\", \"user\": \"Postman\"}\" > состояние чата:\"{\n",
            "  \"response\": [\n",
            "    \"Postman: Your Cristmas Tree.\",\n",
            "    \"Lisa Simpson: I have no problem with that. But I'm concerned with the other tree.\"\n",
            "  ]\n",
            "}\"\n",
            "---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:chat_util_module:get_answer_gpt2_model - старт выполнения\n",
            "DEBUG:chat_util_module:Query: From North Pole.\n",
            "DEBUG:chat_util_module:User: Postman\n",
            "DEBUG:chat_util_module:Обогащенный контекстом запрос: R_3: \"Postman: Your Cristmas Tree.\"; R_2: \"Lisa Simpson: I have no problem with that. But I'm concerned with the other tree.\"; R_1: \"Postman: From North Pole.\"\n",
            "DEBUG:chat_util_module:gpt модели ответ: Lisa Simpson: I have no problem with that. But I'm concerned with the other tree.\n",
            "DEBUG:chat_util_module:Тело ответа gpt2: ['Postman: Your Cristmas Tree.', \"Lisa Simpson: I have no problem with that. But I'm concerned with the other tree.\", 'Postman: From North Pole.', \"Lisa Simpson: I have no problem with that. But I'm concerned with the other tree.\"]\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:51] \"POST /gpt2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 \"/gpt2\" запрос:\"{\"query\": \"From North Pole.\", \"user\": \"Postman\"}\" > состояние чата:\"{\n",
            "  \"response\": [\n",
            "    \"Postman: Your Cristmas Tree.\",\n",
            "    \"Lisa Simpson: I have no problem with that. But I'm concerned with the other tree.\",\n",
            "    \"Postman: From North Pole.\",\n",
            "    \"Lisa Simpson: I have no problem with that. But I'm concerned with the other tree.\"\n",
            "  ]\n",
            "}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Mar/2024 20:54:52] \"DELETE /clear HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}